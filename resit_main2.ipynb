{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "541c282f-43ac-4b65-b030-19e9726f8b71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d3fda31-d9ae-461a-9403-a10a1c2f950c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainfile = 'data/en_ewt-up-train.conllu'\n",
    "devfile = 'data/en_ewt-up-dev.conllu'\n",
    "testfile = 'data/en_ewt-up-test.conllu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6ba9e0-9331-4404-8080-072616672af1",
   "metadata": {},
   "source": [
    "# Read the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "227c1d87-f5de-4f5e-b2df-37b0b4a22ac8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_conll(conllfile):\n",
    "    \"\"\"\n",
    "    This function read and process the conllu file into list of sentences lists.\n",
    "    \"\"\"\n",
    "    with open(conllfile, 'r', encoding='utf8') as infile:\n",
    "        fulllist, sentlist = [],[]\n",
    "        for line in infile:\n",
    "            line = line.strip()\n",
    "            if (line != '\\n') & (line.startswith(\"#\") == False): # Not empty and not commented\n",
    "                sentlist.append(line.split())\n",
    "            if line.startswith(\"#\") == True:\n",
    "                sentlist = [i for i in sentlist if i] # Remove empty list\n",
    "                fulllist.append(sentlist)\n",
    "                sentlist = []\n",
    "                continue\n",
    "        res = [ele for ele in fulllist if ele != []] # remove empty list\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8213978c-b059-470d-b68e-0b67582bdf0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#read the file into list of sentence lists\n",
    "trainlist = read_conll(trainfile)\n",
    "devlist = read_conll(devfile)\n",
    "testlist = read_conll(testfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac429591-0422-4e65-9039-ab7594d94f14",
   "metadata": {},
   "source": [
    "Preprocess the data\n",
    "\n",
    "extract potential features and duplicate sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8387c3da-efe4-4a1e-8d47-b6e9b5bb570e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_list(conlllist):\n",
    "    \"\"\"\n",
    "    This function preprocess the lists into list of sentences list.\n",
    "    Each sentence list is a list of token lists. Each token list have 13 columns.\n",
    "        If a sentence have 0 predicates, the column (list item) 12 and 13 (list[11] and list[12]) are set as None.\n",
    "        If the sentence have multiple predicates, it will be duplicated to align the column number.\n",
    "        If a sentence does not have record on line 11, it will be filled with '_'\n",
    "    \"\"\"\n",
    "    sentlist = []\n",
    "    for sentence in conlllist:\n",
    "        sents = [ [] for _ in range(50) ] # Initialize a large empty list for multiple predicate sentence    \n",
    "        \n",
    "        for x in range(len(sentence)): # replace 'for components in sentence' that brings duplicate removal error\n",
    "            components = []\n",
    "            for y in range(len(sentence[x])):\n",
    "                components.append(str(sentence[x][y]))\n",
    "\n",
    "            # First 11 lines\n",
    "            for i in range(0,10):\n",
    "                try:\n",
    "                    tokendict = {\"ID\":components[0], \"form\":components[1], \"lemma\":components[2], \"upos\":components[3], \"xpos\":components[4], \"feats\":components[5], \"head\":components[6], \n",
    "                             \"deprel\":components[7], \"deps\":components[8], \"misc\":components[9], \"pred\":components[10]}\n",
    "                except IndexError: # Wrong sentence in the dataset that have no column 11\n",
    "                    tokendict['pred'] = '_'\n",
    "\n",
    "            # If sentence have no predicate: assign the values '_'\n",
    "            if len(components) <= 11: \n",
    "                tokendict['V'], tokendict['ARG'] ,tokendict['dup'] = '_','_','_'\n",
    "                sents[0].append(tokendict)\n",
    "\n",
    "            # Sentence have one or more predicate\n",
    "            if len(components) > 11: \n",
    "                dup = len(components)-11 # Times for dpulication\n",
    "                for k in range(0, dup):\n",
    "                    tokendictk = copy.deepcopy(tokendict)\n",
    "                    tokendictk['dup'] = k\n",
    "                    ARGV = components[k+11]\n",
    "                    # Following conditons change 'pred' (and ARG, V also) entry for duplicated sentence\n",
    "                    if ARGV == 'V':\n",
    "                        tokendictk['V'],tokendictk['ARG'] = 'V','_'\n",
    "                        try:\n",
    "                            tokendictk['pred'] = sentence[int(tokendictk['ID'])-1][10]\n",
    "                        except IndexError:\n",
    "                            print('The following sentence contains error:',sentence)\n",
    "                            continue\n",
    "                    if (ARGV != 'V') & (ARGV != '_'):\n",
    "                        tokendictk['ARG'],tokendictk['V'],tokendictk['pred'] = ARGV,'_','_'\n",
    "                    if ARGV == '_':\n",
    "                        tokendictk['V'],tokendictk['ARG'],tokendictk['pred'] = '_','_','_'\n",
    "                    sents[k].append(tokendictk)\n",
    "\n",
    "\n",
    "        res = [ele for ele in sents if ele != []] # remove empty list\n",
    "        sentlist += res\n",
    "\n",
    "    return sentlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "caaf87fb-bddb-43eb-b088-e12032c5ceec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following sentence contains error: [['1', 'I', 'I', 'PRON', 'PRP', 'Case=Nom|Number=Sing|Person=1|PronType=Prs', '2', 'nsubj', '2:nsubj|9.1:nsubj|10:nsubj', '_', '_', 'ARG0', '_', '_'], ['2', 'wish', 'wish', 'VERB', 'VBP', 'Mood=Ind|Tense=Pres|VerbForm=Fin', '0', 'root', '0:root', '_', 'wish.01', 'V', '_', '_'], ['3', 'all', 'all', 'DET', 'DT', '_', '2', 'iobj', '2:iobj', '_', '_', 'ARG2', '_', '_'], ['4', 'happy', 'happy', 'ADJ', 'JJ', 'Degree=Pos', '5', 'amod', '5:amod', '_', '_', '_', 'ARGM-ADJ', '_'], ['5', 'holidays', 'holiday', 'NOUN', 'NNS', 'Number=Plur', '2', 'obj', '2:obj', 'SpaceAfter=No', 'holiday.01', 'ARG1', 'V', '_'], ['6', ',', ',', 'PUNCT', ',', '_', '10', 'punct', '9.1:punct|10:punct', '_', '_', '_', '_', '_'], ['7', 'and', 'and', 'CCONJ', 'CC', '_', '10', 'cc', '9.1:cc|10:cc', '_', '_', '_', '_', '_'], ['8', 'moreso', 'moreso', 'ADV', 'RB', '_', '10', 'orphan', '9.1:advmod', 'SpaceAfter=No', '_', '_', '_', '_'], ['9', ',', ',', 'PUNCT', ',', '_', '10', 'punct', '9.1:punct|10:punct', '_', '_', '_', '_', '_'], ['9.1', 'wish', 'wish', 'VERB', 'VBP', 'Mood=Ind|Tense=Pres|VerbForm=Fin', '_', '_', '2:conj:and', 'CopyOf=2'], ['10', 'peace', 'peace', 'NOUN', 'NN', 'Number=Sing', '2', 'conj', '2:conj:and|9.1:obj', '_', 'peace.01', 'ARG1', '_', 'V'], ['11', 'on', 'on', 'ADP', 'IN', '_', '12', 'case', '12:case', '_', '_', '_', '_', '_'], ['12', 'earth', 'earth', 'NOUN', 'NN', 'Number=Sing', '10', 'nmod', '10:nmod:on', 'SpaceAfter=No', '_', '_', '_', 'ARG1'], ['13', '.', '.', 'PUNCT', '.', '_', '2', 'punct', '2:punct', '_', '_', '_', '_', '_']]\n"
     ]
    }
   ],
   "source": [
    "preprocessed_train = preprocess_list(trainlist)\n",
    "preprocessed_dev = preprocess_list(devlist)\n",
    "preprocessed_test = preprocess_list(testlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c210c336-2b5f-491a-ae32-351eb87b75dd",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57eb4a47-ad89-4698-887b-cc2eb09779cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "def create_tree(sent):\n",
    "    \"\"\"\n",
    "    This function creates a dependency tree between head and target token.\n",
    "\n",
    "    Para::sent: A list of token dictionaries, each representing a token\n",
    "                         and its information.\n",
    "    \n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "    for t_dict in sent:\n",
    "        if t_dict['head'] != 0:\n",
    "            start, end = int(t_dict['head']), int(t_dict['ID']) #find starting and end point of the tree\n",
    "            G.add_edge(start,end)\n",
    "            G.edges[start, end]['deprel'] = t_dict['deprel']\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e18143e6-bd83-488f-acc7-f8fe98de241f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_with_deprel_and_depth(G, tokenID, predID):\n",
    "    \"\"\"\n",
    "    This function finds the shortest dependency path from the target token to the target predicate\n",
    "    Returns a dependency path str.\n",
    "    \n",
    "    Para::G: dependency graph from token to head\n",
    "    Para::tokenID: target token ID\n",
    "    para::predID: target predicate ID\n",
    "    \"\"\"\n",
    "    path_string = \"\"\n",
    "    path = nx.shortest_path(G, source=tokenID, target=predID)\n",
    "    for i in range(len(path)-1):\n",
    "        \n",
    "        if len(nx.shortest_path(G, source=0, target=path[i])) > len(nx.shortest_path(G, source=0, target=path[i+1])):\n",
    "            path_string += \"U\" # UP\n",
    "        else:\n",
    "            path_string += \"D\" # DOWN\n",
    "\n",
    "        path_string = path_string + G.edges[path[i],path[i+1]]['deprel'] + '_'\n",
    "        \n",
    "    return path_string[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f49e8608-7388-4723-a7fc-e7bc1eabf26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_path_to_pred(sent):\n",
    "    \"\"\"\n",
    "    This function \n",
    "    1)adds predicate lemma to the end of dependecy path string\n",
    "    2)add the dependency path feature of tokens to token dicts\n",
    "    \n",
    "    Para::sent:A list of token dictionaries, each representing a token and its information.\n",
    "    \"\"\"\n",
    "    dep_tree = create_tree(sent)\n",
    "    predID = -1\n",
    "    pred_lema = ''\n",
    "    for token in sent:\n",
    "        if token['V'] == 'V':\n",
    "            predID = int(token['ID'])\n",
    "            pred_lema = token['lemma']\n",
    "            break\n",
    "    \n",
    "    for token in sent:\n",
    "        if predID == -1: # no predicate in the sentence\n",
    "            token['path_to_pred'] = '_'\n",
    "        else:\n",
    "            token['path_to_pred'] = path_with_deprel_and_depth(dep_tree,int(token['ID']),predID) + f\"_{pred_lema}\"\n",
    "            \n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3a218f9-d9ee-46f9-b223-7f962a2b540f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_feature_label(preprocessed_list, selected_features):\n",
    "    \"\"\"\n",
    "    Extract features and labels for training, keeping only selected features.\n",
    "    \n",
    "    Parametser:\n",
    "    preprocessed_list (list): List of lists containing token dictionaries.\n",
    "    selected_features (list): List of feature names to be kept in the extracted features.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: A tuple containing two lists - features and labels.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    gold_labels = []\n",
    "\n",
    "    # Flatten the list of lists into a single list of token dictionaries\n",
    "    token_list = [token_dict for sent_list in preprocessed_list for token_dict in sent_list]\n",
    "\n",
    "    for token_dict in token_list:\n",
    "        newdict = {feature: token_dict[feature] for feature in selected_features if feature in token_dict}\n",
    "        features.append(newdict)\n",
    "        gold_labels.append(token_dict[\"ARG\"])\n",
    "\n",
    "    return features, gold_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c9ba95e-1fc4-4ed7-a979-92105cf3f627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "463d9eca003742f6bf615471a34659e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42466 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'upos': 'PROPN', 'path_to_pred': 'Dparataxis_kill'} _\n",
      "{'upos': 'PUNCT', 'path_to_pred': 'Upunct_Dparataxis_kill'} _\n",
      "{'upos': 'PROPN', 'path_to_pred': 'Uflat_Dparataxis_kill'} _\n",
      "{'upos': 'PUNCT', 'path_to_pred': 'Upunct_Dparataxis_kill'} _\n",
      "{'upos': 'ADJ', 'path_to_pred': 'Uamod_Unsubj_kill'} _\n",
      "{'upos': 'NOUN', 'path_to_pred': 'Unsubj_kill'} ARG0\n",
      "{'upos': 'VERB', 'path_to_pred': '_kill'} _\n",
      "{'upos': 'PROPN', 'path_to_pred': 'Uobj_kill'} ARG1\n",
      "{'upos': 'PROPN', 'path_to_pred': 'Uflat_Uobj_kill'} _\n",
      "{'upos': 'PROPN', 'path_to_pred': 'Uflat_Uobj_kill'} _\n"
     ]
    }
   ],
   "source": [
    "# apply to training data and extract training feature\n",
    "    \n",
    "selected_features = [\"upos\", \"path_to_pred\"]\n",
    "\n",
    "sents_tr=[]\n",
    "for sents in tqdm(preprocessed_train):\n",
    "    sent_all_feat = find_path_to_pred(sents)\n",
    "    sents_tr.append(sent_all_feat)\n",
    "\n",
    "train_features, train_gold = extract_feature_label(sents_tr, selected_features)\n",
    "\n",
    "for f,g in zip(train_features[:10], train_gold[:10]):\n",
    "    print(f,g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3450d436-3344-4cc2-8dde-54dd549c02b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b878448ed320458cab32cda85ce86a67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5328 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'upos': 'PRON', 'path_to_pred': 'Dadvcl_morph'} _\n",
      "{'upos': 'SCONJ', 'path_to_pred': 'Umark_morph'} _\n",
      "{'upos': 'PROPN', 'path_to_pred': 'Unsubj_morph'} ARG1\n",
      "{'upos': 'VERB', 'path_to_pred': '_morph'} _\n",
      "{'upos': 'ADP', 'path_to_pred': 'Ucase_Uobl_morph'} _\n",
      "{'upos': 'PROPN', 'path_to_pred': 'Uobl_morph'} ARG2\n",
      "{'upos': 'PUNCT', 'path_to_pred': 'Upunct_morph'} _\n",
      "{'upos': 'PRON', 'path_to_pred': 'Dadvcl_expand'} _\n",
      "{'upos': 'SCONJ', 'path_to_pred': 'Umark_expand'} _\n",
      "{'upos': 'PROPN', 'path_to_pred': 'Unsubj_expand'} ARG0\n"
     ]
    }
   ],
   "source": [
    "# apply to training data and extract training feature\n",
    "sents_te=[]\n",
    "for sents in tqdm(preprocessed_test):\n",
    "    sent_all_feat = find_path_to_pred(sents)\n",
    "    sents_te.append(sent_all_feat)\n",
    "\n",
    "test_features, test_gold = extract_feature_label(sents_te, selected_features)\n",
    "\n",
    "for f,g in zip(test_features[:10], test_gold[:10]):\n",
    "    print(f,g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cd81a3-b9a8-4078-b589-c9e336c1a2a9",
   "metadata": {},
   "source": [
    "## model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af32643b-4732-4129-b743-c4175bfd93d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "def create_log_classifier(train_features, train_targets, max_iter):\n",
    "    logreg = LogisticRegression(max_iter=max_iter)\n",
    "    vec = DictVectorizer()\n",
    "    features_vectorized = vec.fit_transform(train_features)\n",
    "    model = logreg.fit(features_vectorized, train_targets) \n",
    "    return model, vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a310f5b9-adf7-40e6-8c0a-00e2cad81b59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def create_log_model(train_features, gold_labels, max_iter):\n",
    "#     logreg = LogisticRegression(max_iter)\n",
    "#     vec = DictVectorizer()\n",
    "#     features_vectorized = vec.fit_transform(train_features)\n",
    "#     model = logreg.fit(features_vectorized, gold_labels)\n",
    "#     return model, vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96674c1a-de34-45c4-a6f6-97e472104e2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_single, vec_single = create_log_classifier(train_features, train_gold, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673fe791-9f6f-4d0f-a317-57e48c29db3d",
   "metadata": {},
   "source": [
    "## model predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb6acdb-56d3-43cd-9d25-02bace19456f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_data(model, vec, test_features):  \n",
    "    features = vec.transform(test_features)\n",
    "    predictions = model.predict(test_features)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b00136-a362-4276-b39b-200dced58ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_pred = classify_data(model_single, vec_single, test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e30de07-6604-4841-a153-8b9bcc3a3cec",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d96141-6ae6-4ac4-98dc-0ae7bddeb9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "overall_report = classification_report(test_gold, single_pred, digits = 7, target_names = [\"True\", \"False\"])\n",
    "print(overall_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053d6349-e8eb-4c66-a7ef-644806a02957",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_set = set(sorted(test_gold))\n",
    "label_report = classification_report(test_gold, single_pred, digits = 7, target_names = label_set)\n",
    "print(label_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a466a2ec-f95e-40dd-8294-2174bb0b6db8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'upos': 'PRON', 'path_to_pred': 'Dadvcl_morph'} _\n",
      "{'upos': 'SCONJ', 'path_to_pred': 'Umark_morph'} _\n",
      "{'upos': 'PROPN', 'path_to_pred': 'Unsubj_morph'} ARG1\n",
      "{'upos': 'VERB', 'path_to_pred': '_morph'} _\n",
      "{'upos': 'ADP', 'path_to_pred': 'Ucase_Uobl_morph'} _\n",
      "{'upos': 'PROPN', 'path_to_pred': 'Uobl_morph'} ARG2\n",
      "{'upos': 'PUNCT', 'path_to_pred': 'Upunct_morph'} _\n",
      "{'upos': 'PRON', 'path_to_pred': 'Dadvcl_expand'} _\n",
      "{'upos': 'SCONJ', 'path_to_pred': 'Umark_expand'} _\n",
      "{'upos': 'PROPN', 'path_to_pred': 'Unsubj_expand'} ARG0\n"
     ]
    }
   ],
   "source": [
    "selected_features = [\"upos\", \"dep_path_lemma\"]\n",
    "\n",
    "train_features, train_gold = extract_feature_label(sents_all_feat, selected_features)\n",
    "\n",
    "\n",
    "for f,g in zip(features[:10], gold_labels[:10]):\n",
    "    print(f,g)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
