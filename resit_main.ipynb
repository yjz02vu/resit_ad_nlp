{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab8c0e0e-54e0-49bd-8aae-7c3932f9f61a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d9db276-a61a-42e4-889a-87e08d35d98e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainfile = 'data/en_ewt-up-train.conllu'\n",
    "devfile = 'data/en_ewt-up-dev.conllu'\n",
    "testfile = 'data/en_ewt-up-test.conllu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0b003c-b177-48ed-8190-1f0077213fbb",
   "metadata": {},
   "source": [
    "## Read the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bce9138b-e4a2-49c1-88dc-0c7855fd1ce6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_conll(conllfile):\n",
    "    \"\"\"\n",
    "    This function read and process the conllu file into list of sentences lists.\n",
    "    \"\"\"\n",
    "    with open(conllfile, 'r', encoding='utf8') as infile:\n",
    "        fulllist, sentlist = [],[]\n",
    "        for line in infile:\n",
    "            line = line.strip()\n",
    "            if (line != '\\n') & (line.startswith(\"#\") == False): # Not empty and not commented\n",
    "                sentlist.append(line.split())\n",
    "            if line.startswith(\"#\") == True:\n",
    "                sentlist = [i for i in sentlist if i] # Remove empty list\n",
    "                fulllist.append(sentlist)\n",
    "                sentlist = []\n",
    "                continue\n",
    "        res = [ele for ele in fulllist if ele != []] # remove empty list\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "443ea3e7-3081-4710-bb8d-43667cd7027d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#read the file into list of sentence lists\n",
    "trainlist = read_conll(trainfile)\n",
    "devlist = read_conll(devfile)\n",
    "testlist = read_conll(testfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70327a68-c7b9-4306-a8a5-7bf159aa787d",
   "metadata": {},
   "source": [
    "## Preprocess the data\n",
    "extract potential features and duplicate sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56ebe844-a75d-41a0-8258-bc7e6874e472",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_list(conlllist):\n",
    "    \"\"\"\n",
    "    This function preprocess the lists into list of sentences list.\n",
    "    Each sentence list is a list of token lists. Each token list have 13 columns.\n",
    "        If a sentence have 0 predicates, the column (list item) 12 and 13 (list[11] and list[12]) are set as None.\n",
    "        If the sentence have multiple predicates, it will be duplicated to align the column number.\n",
    "        If a sentence does not have record on line 11, it will be filled with '_'\n",
    "    \"\"\"\n",
    "    sentlist = []\n",
    "    for sentence in conlllist:\n",
    "        sents = [ [] for _ in range(50) ] # Initialize a large empty list for multiple predicate sentence    \n",
    "        \n",
    "        for x in range(len(sentence)): # replace 'for components in sentence' that brings duplicate removal error\n",
    "            components = []\n",
    "            for y in range(len(sentence[x])):\n",
    "                components.append(str(sentence[x][y]))\n",
    "\n",
    "            # First 11 lines\n",
    "            for i in range(0,10):\n",
    "                try:\n",
    "                    tokendict = {\"ID\":components[0], \"form\":components[1], \"lemma\":components[2], \"upos\":components[3], \"xpos\":components[4], \"feats\":components[5], \"head\":components[6], \n",
    "                             \"deprel\":components[7], \"deps\":components[8], \"misc\":components[9], \"pred\":components[10]}\n",
    "                except IndexError: # Wrong sentence in the dataset that have no column 11\n",
    "                    tokendict['pred'] = '_'\n",
    "\n",
    "            # If sentence have no predicate: assign the values '_'\n",
    "            if len(components) <= 11: \n",
    "                tokendict['V'], tokendict['ARG'] ,tokendict['dup'] = '_','_','_'\n",
    "                sents[0].append(tokendict)\n",
    "\n",
    "            # Sentence have one or more predicate\n",
    "            if len(components) > 11: \n",
    "                dup = len(components)-11 # Times for dpulication\n",
    "                for k in range(0, dup):\n",
    "                    tokendictk = copy.deepcopy(tokendict)\n",
    "                    tokendictk['dup'] = k\n",
    "                    ARGV = components[k+11]\n",
    "                    # Following conditons change 'pred' (and ARG, V also) entry for duplicated sentence\n",
    "                    if ARGV == 'V':\n",
    "                        tokendictk['V'],tokendictk['ARG'] = 'V','_'\n",
    "                        try:\n",
    "                            tokendictk['pred'] = sentence[int(tokendictk['ID'])-1][10]\n",
    "                        except IndexError:\n",
    "                            print('The following sentence contains error:',sentence)\n",
    "                            continue\n",
    "                    if (ARGV != 'V') & (ARGV != '_'):\n",
    "                        tokendictk['ARG'],tokendictk['V'],tokendictk['pred'] = ARGV,'_','_'\n",
    "                    if ARGV == '_':\n",
    "                        tokendictk['V'],tokendictk['ARG'],tokendictk['pred'] = '_','_','_'\n",
    "                    sents[k].append(tokendictk)\n",
    "\n",
    "\n",
    "        res = [ele for ele in sents if ele != []] # remove empty list\n",
    "        sentlist += res\n",
    "\n",
    "    return sentlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12f300d1-1f28-4c69-ba80-788f14624da6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following sentence contains error: [['1', 'I', 'I', 'PRON', 'PRP', 'Case=Nom|Number=Sing|Person=1|PronType=Prs', '2', 'nsubj', '2:nsubj|9.1:nsubj|10:nsubj', '_', '_', 'ARG0', '_', '_'], ['2', 'wish', 'wish', 'VERB', 'VBP', 'Mood=Ind|Tense=Pres|VerbForm=Fin', '0', 'root', '0:root', '_', 'wish.01', 'V', '_', '_'], ['3', 'all', 'all', 'DET', 'DT', '_', '2', 'iobj', '2:iobj', '_', '_', 'ARG2', '_', '_'], ['4', 'happy', 'happy', 'ADJ', 'JJ', 'Degree=Pos', '5', 'amod', '5:amod', '_', '_', '_', 'ARGM-ADJ', '_'], ['5', 'holidays', 'holiday', 'NOUN', 'NNS', 'Number=Plur', '2', 'obj', '2:obj', 'SpaceAfter=No', 'holiday.01', 'ARG1', 'V', '_'], ['6', ',', ',', 'PUNCT', ',', '_', '10', 'punct', '9.1:punct|10:punct', '_', '_', '_', '_', '_'], ['7', 'and', 'and', 'CCONJ', 'CC', '_', '10', 'cc', '9.1:cc|10:cc', '_', '_', '_', '_', '_'], ['8', 'moreso', 'moreso', 'ADV', 'RB', '_', '10', 'orphan', '9.1:advmod', 'SpaceAfter=No', '_', '_', '_', '_'], ['9', ',', ',', 'PUNCT', ',', '_', '10', 'punct', '9.1:punct|10:punct', '_', '_', '_', '_', '_'], ['9.1', 'wish', 'wish', 'VERB', 'VBP', 'Mood=Ind|Tense=Pres|VerbForm=Fin', '_', '_', '2:conj:and', 'CopyOf=2'], ['10', 'peace', 'peace', 'NOUN', 'NN', 'Number=Sing', '2', 'conj', '2:conj:and|9.1:obj', '_', 'peace.01', 'ARG1', '_', 'V'], ['11', 'on', 'on', 'ADP', 'IN', '_', '12', 'case', '12:case', '_', '_', '_', '_', '_'], ['12', 'earth', 'earth', 'NOUN', 'NN', 'Number=Sing', '10', 'nmod', '10:nmod:on', 'SpaceAfter=No', '_', '_', '_', 'ARG1'], ['13', '.', '.', 'PUNCT', '.', '_', '2', 'punct', '2:punct', '_', '_', '_', '_', '_']]\n"
     ]
    }
   ],
   "source": [
    "preprocessed_train = preprocess_list(trainlist)\n",
    "preprocessed_dev = preprocess_list(devlist)\n",
    "preprocessed_test = preprocess_list(testlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af64116d-7b2a-423e-8afd-3dfa8f1bfe7e",
   "metadata": {},
   "source": [
    "## Feature Extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef1aedc9-785d-4a07-947a-296fcca5074a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52a2bf6c-4a41-4658-8d83-956616d12827",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def merge_sent (sent):\n",
    "#     \"\"\"\n",
    "#     merge the token into sentences for parsing\n",
    "#     return a dict with inform of sentence, the predicate being predicted and its lemma\n",
    "    \n",
    "#     sent::sentence list containing token dicts \n",
    "#     \"\"\"\n",
    "\n",
    "#     merged_dict = {}\n",
    "#     merged_sent = \"\"\n",
    "#     for n, i in enumerate(sent):\n",
    "#         if n != len(sent) -1:\n",
    "#             merged_sent += sent[n]['form'] + ' ' \n",
    "#         else:\n",
    "#             merged_sent += sent[n]['form']\n",
    "            \n",
    "#         if sent[n]['pred'] != \"_\":\n",
    "#             merged_dict[\"pred\"] = sent[n]['form']\n",
    "#             merged_dict[\"lemma\"] = sent[n]['lemma']\n",
    "#     merged_dict[\"sent\"] = merged_sent\n",
    "\n",
    "#     return merged_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06a5df27-529f-4a07-8d70-d5794f60b905",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def merge_sent_token (sent):\n",
    "    \"\"\"\n",
    "    merge the token into sentences for parsing\n",
    "    return a dict with inform of sentence, the predicate being predicted and its lemma\n",
    "    \n",
    "    sent::sentence list containing token dicts \n",
    "    \"\"\"\n",
    "\n",
    "    merged_dict = {}\n",
    "    merged_sent = \"\"\n",
    "    for n, i in enumerate(sent):\n",
    "        if n != len(sent) -1:\n",
    "            merged_sent += sent[n]['form'] + ' ' \n",
    "        else:\n",
    "            merged_sent += sent[n]['form']\n",
    "            \n",
    "        if sent[n]['pred'] != \"_\":\n",
    "            merged_dict[\"pred\"] = sent[n]['form']\n",
    "            merged_dict[\"lemma\"] = sent[n]['lemma']\n",
    "\n",
    "            \n",
    "    merged_dict[\"sent\"] = merged_sent\n",
    "    merged_dict[\"token_l\"] = [word[\"form\"] for word in sent]\n",
    "    \n",
    "#     check the predicate\n",
    "    if \"pred\" not in merged_dict:\n",
    "        merged_dict[\"pred\"] = None\n",
    "        merged_dict[\"lemma\"] = None\n",
    "\n",
    "    return merged_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1065fb06-40ef-4570-b2f8-e4f11e4cd1bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sentence4 =  preprocessed_train[373] \n",
    "# merged_dict4 = merge_sent_token(sentence4)\n",
    "# print(merged_dict4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7933e209-382c-4b79-9943-6290595b1d34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #merge back into sentence to be parsed, create a dictionary save sentence and the predicate of each sentence\n",
    "# #merged_dict[\"pred\"] stores the predicate being predicted, merged_dict[\"lemma\"] stores the lemma of the predicate\n",
    "# merged_l = [] #list of dictionaroies, each dict has merged sent and responding target predicate\n",
    "\n",
    "# for k, sent in enumerate(preprocessed_try):\n",
    "#     merged_dict = {}\n",
    "#     merged_sent = \"\"\n",
    "#     for n, i in enumerate(sent):\n",
    "#         if n != len(sent) -1:\n",
    "#             merged_sent += sent[n]['form'] + ' ' \n",
    "#         else:\n",
    "#             merged_sent += sent[n]['form']\n",
    "            \n",
    "#         if sent[n]['pred'] != \"_\":\n",
    "#             merged_dict[\"pred\"] = sent[n]['form']\n",
    "#             merged_dict[\"lemma\"] = sent[n]['lemma']\n",
    "#     merged_dict[\"sent\"] = merged_sent\n",
    "    \n",
    "#     merged_l.append(merged_dict)\n",
    "        \n",
    "        \n",
    "# print(merged_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f328106f-95a1-4a7d-8156-e65b0518e94b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def dep_path_2lemma (s, nlp):\n",
    "#     \"\"\"\n",
    "#     create the dependency path from target_token to target_predicate with the predicate lemma\n",
    "#     return a sentence list with dependency path feature of tokens\n",
    "    \n",
    "#     sent::dict of merged sentence\n",
    "#     nlp:: spacy parse\n",
    "#     \"\"\"\n",
    "#     sent = s[\"sent\"]\n",
    "#     token_text = sent.split(' ') #token_text is the same with dataset\n",
    "    \n",
    "#     doc = nlp (sent)\n",
    "    \n",
    "#     #get pred and its lemma of the sentence\n",
    "#     t_pred = s['pred'] \n",
    "#     t_lemma = s['lemma']\n",
    "    \n",
    "#     #find the nlp_pred corresponding to pred_text\n",
    "#     for t in doc:\n",
    "#         if t.text == t_pred:\n",
    "#             t_pred = t \n",
    "#         else:\n",
    "#             continue\n",
    "#     #find the nlp_token corresponding to token_text\n",
    "#     t_token = None \n",
    "    \n",
    "#     token_deps = []\n",
    "#     for i, t in enumerate(doc):\n",
    "#         if t.text == token_text[i]:\n",
    "#             t_token = t\n",
    "#         else:\n",
    "#             print(f\"unmatched nlp_token {t} and token_text {token_text[i]}\")\n",
    "#             continue\n",
    "            \n",
    "#         #find ancestors of each token\n",
    "#         token_ancestors=[]\n",
    "#         for anc in t_token.ancestors:\n",
    "#             token_ancestors.append((anc, anc.dep_))#record each anc and the dep\n",
    "            \n",
    "#         # path from the target token to the ancestors\n",
    "#         token_ancestors.insert(0,(t_token, t_token.dep_))\n",
    "            \n",
    "#         pred_ancestors = []\n",
    "#         for anc in t_pred.ancestors:\n",
    "#             pred_ancestors.append((anc, anc.dep_))\n",
    "#         # path from the target predict to the ancestors\n",
    "#         pred_ancestors.insert(0,(t_pred, t_pred.dep_))\n",
    " \n",
    "#         # Create path to the first common ancestor\n",
    "#         common_ancs = set(token_ancestors).intersection(pred_ancestors)\n",
    "        \n",
    "#         token_path = []\n",
    "#         pred_path = []\n",
    "        \n",
    "#         for t in token_ancestors: #add token and dep_ until meets the common_anc\n",
    "#             if t in common_ancs:\n",
    "#                 break\n",
    "#             token_path.append(t) \n",
    "            \n",
    "#         for t in pred_ancestors:\n",
    "#             if t in common_ancs:\n",
    "#                 break\n",
    "#             pred_path.append(t)\n",
    "            \n",
    "#         # revers pred_path for order \"t_taget - t_pred\"\n",
    "#         pred_path.reverse()\n",
    "        \n",
    "#         #path list with tuple of each token and dep\n",
    "#         dep_path_l= token_path + pred_path\n",
    "        \n",
    "#         #add lemma of root\n",
    "#         if dep_path_l == []:\n",
    "#             dep_path_l.append((t_token,''))\n",
    "            \n",
    "#         #path string\n",
    "#         dep_path = ''\n",
    "#         for t,p in dep_path_l:\n",
    "#             dep_path += p + '_' \n",
    "#         #add the predicate lemma\n",
    "#         dep_path += t_lemma\n",
    "        \n",
    "#         token_deps.append(dep_path) #list of dep path of each token to t_pred \n",
    "#     return token_deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7c736251-6336-49de-8105-f350fab700ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dep_path_2lemma_c (s, nlp):\n",
    "    \"\"\"\n",
    "    create the dependency path from target_token to target_predicate with the predicate lemma\n",
    "    return a sentence list with dependency path feature of tokens\n",
    "    \n",
    "    sent::dict of merged sentence\n",
    "    nlp:: spacy parse\n",
    "    \"\"\"\n",
    "    sent = s[\"sent\"]\n",
    "    token_text = sent.split(' ') #token_text is the same with dataset\n",
    "    \n",
    "    doc = nlp (sent)\n",
    "    \n",
    "    #get pred and its lemma of the sentence\n",
    "    \n",
    "        \n",
    "    t_pred = s['pred'] \n",
    "    t_lemma = s['lemma']\n",
    "        \n",
    "    if t_pred == None and t_lemma == None:\n",
    "        \n",
    "        n = len(token_text)\n",
    "        token_deps = [\"Unknown\"] *n\n",
    "        return token_deps\n",
    "        \n",
    "    \n",
    "    #find the nlp_pred corresponding to pred_text\n",
    "    for t in doc:\n",
    "        if t.text == t_pred:\n",
    "            t_pred = t \n",
    "        else:\n",
    "            continue\n",
    "    #find the nlp_token corresponding to token_text\n",
    "    t_token = None \n",
    "    \n",
    "    token_deps = []\n",
    "    for i, t in enumerate(doc):\n",
    "        try:\n",
    "            if t.text == token_text[i]:\n",
    "                t_token = t\n",
    "        except IndexError:\n",
    "            print(f\"unmatched nlp_token and token_text\")\n",
    "            \n",
    "            \n",
    "        \n",
    "            \n",
    "        #find ancestors of each token\n",
    "        token_ancestors=[]\n",
    "        for anc in t_token.ancestors:\n",
    "            token_ancestors.append((anc, anc.dep_))#record each anc and the dep\n",
    "            \n",
    "        # path from the target token to the ancestors\n",
    "        token_ancestors.insert(0,(t_token, t_token.dep_))\n",
    "            \n",
    "        pred_ancestors = []\n",
    "        \n",
    "        try:\n",
    "            for anc in t_pred.ancestors:\n",
    "                pred_ancestors.append((anc, anc.dep_))\n",
    "                \n",
    "        except AttributeError:\n",
    "            print(AttributeError)\n",
    "        # path from the target predict to the ancestors\n",
    "        pred_ancestors.insert(0,(t_pred, t_pred.dep_))\n",
    " \n",
    "        # Create path to the first common ancestor\n",
    "        common_ancs = set(token_ancestors).intersection(pred_ancestors)\n",
    "        \n",
    "        token_path = []\n",
    "        pred_path = []\n",
    "        \n",
    "        for t in token_ancestors: #add token and dep_ until meets the common_anc\n",
    "            if t in common_ancs:\n",
    "                break\n",
    "            token_path.append(t) \n",
    "            \n",
    "        for t in pred_ancestors:\n",
    "            if t in common_ancs:\n",
    "                break\n",
    "            pred_path.append(t)\n",
    "            \n",
    "        # revers pred_path for order \"t_taget - t_pred\"\n",
    "        pred_path.reverse()\n",
    "        \n",
    "        #path list with tuple of each token and dep\n",
    "        dep_path_l= token_path + pred_path\n",
    "        \n",
    "        #add lemma of root\n",
    "        if dep_path_l == []:\n",
    "            dep_path_l.append((t_token,''))\n",
    "            \n",
    "        #path string\n",
    "        dep_path = ''\n",
    "        for t,p in dep_path_l:\n",
    "            dep_path += p + '_' \n",
    "        #add the predicate lemma\n",
    "        dep_path += t_lemma\n",
    "        \n",
    "        token_deps.append(dep_path) #list of dep path of each token to t_pred \n",
    "    return token_deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "64e50471-5919-49d4-8493-13c56d8971a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unknown\n",
      "unknown\n",
      "unknown\n",
      "unknown\n",
      "unknown\n",
      "unknown\n",
      "unknown\n",
      "unknown\n",
      "unknown\n",
      "unknown\n",
      "unknown\n",
      "unknown\n",
      "unknown\n",
      "unknown\n",
      "unknown\n",
      "unknown\n",
      "unknown\n"
     ]
    }
   ],
   "source": [
    "s =  preprocessed_train[520]\n",
    "sent_dict = merge_sent_token (s)\n",
    "sent = sent_dict[\"sent\"]\n",
    "token_text = sent.split(' ') #token_text is the same with dataset\n",
    "    \n",
    "doc = nlp (sent)\n",
    "    \n",
    "    #get pred and its lemma of the sentence\n",
    "    \n",
    "        \n",
    "t_pred = sent_dict['pred'] \n",
    "t_lemma = sent_dict['lemma']\n",
    "        \n",
    "# if t_pred == None and t_lemma == None:\n",
    "        \n",
    "#     n = len(token_text)\n",
    "#     token_deps = [\"Unknown\"] *n\n",
    "#     # return token_deps\n",
    "        \n",
    "    \n",
    "    #find the nlp_pred corresponding to pred_text\n",
    "for t in doc:\n",
    "    if t.text == t_pred:\n",
    "        t_pred = t \n",
    "        print(t_pred,type(t_pred))\n",
    "    else:\n",
    "        print(\"unknown\")\n",
    "        continue\n",
    "    #find the nlp_token corresponding to token_text\n",
    "# t_token = None \n",
    "# print(doc)\n",
    "# print(token_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e28157e4-4fd5-4b7a-9bde-2c7b17a6b975",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_features_sent(preprocessed_list):\n",
    "    \"\"\"\n",
    "    create extra features of dependency path to the predicate as well as the predicate lemmaremove the festures not needed\n",
    "    return a list of sentence lists containing all token with wanted features\n",
    "    \n",
    "    preprocessed_list:a list of sentence lists containing all token with features from the dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    feat_4sents = []\n",
    "    \n",
    "    for sentence in tqdm(preprocessed_list):\n",
    "        sent_dict = merge_sent_token (sentence)\n",
    "        sent = sent_dict[\"sent\"]\n",
    "        # print(sent)\n",
    "        # break\n",
    "        token_deps = dep_path_2lemma_c (sent_dict ,nlp)\n",
    "        # print(token_deps)\n",
    "\n",
    "        #write the feature into token_dict\n",
    "        try:\n",
    "            for feature, token in zip(token_deps, sentence):\n",
    "                token[\"dep_path_lemma\"] = feature\n",
    "        except IndexError: \n",
    "            # Handle the case when the zip runs out of items in one of the lists\n",
    "            for token in sentence:\n",
    "                if \"dep_path_lemma\" not in token:\n",
    "                    token[\"dep_path_lemma\"] = \"_\"\n",
    "                    print(f\"dep_path not extracted for {token} in {sent}\")\n",
    "    \n",
    "        feat_4sents.append(sentence)\n",
    "    return (feat_4sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "43cb26fd-13a6-4be8-b04b-e695cf046b1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee18458051f14ab1b9753cdef112b397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42466 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unmatched nlp_token and token_text\n",
      "unmatched nlp_token and token_text\n",
      "unmatched nlp_token and token_text\n",
      "unmatched nlp_token and token_text\n",
      "unmatched nlp_token and token_text\n",
      "unmatched nlp_token and token_text\n",
      "unmatched nlp_token and token_text\n",
      "unmatched nlp_token and token_text\n",
      "unmatched nlp_token and token_text\n",
      "unmatched nlp_token and token_text\n",
      "unmatched nlp_token and token_text\n",
      "unmatched nlp_token and token_text\n",
      "unmatched nlp_token and token_text\n",
      "unmatched nlp_token and token_text\n",
      "unmatched nlp_token and token_text\n",
      "unmatched nlp_token and token_text\n",
      "unmatched nlp_token and token_text\n",
      "unmatched nlp_token and token_text\n",
      "unmatched nlp_token and token_text\n",
      "unmatched nlp_token and token_text\n",
      "unmatched nlp_token and token_text\n",
      "unmatched nlp_token and token_text\n",
      "unmatched nlp_token and token_text\n",
      "unmatched nlp_token and token_text\n",
      "unmatched nlp_token and token_text\n",
      "unmatched nlp_token and token_text\n",
      "unmatched nlp_token and token_text\n",
      "unmatched nlp_token and token_text\n",
      "unmatched nlp_token and token_text\n",
      "unmatched nlp_token and token_text\n",
      "unmatched nlp_token and token_text\n",
      "unmatched nlp_token and token_text\n",
      "unmatched nlp_token and token_text\n",
      "unmatched nlp_token and token_text\n",
      "unmatched nlp_token and token_text\n",
      "unmatched nlp_token and token_text\n",
      "unmatched nlp_token and token_text\n",
      "unmatched nlp_token and token_text\n",
      "unmatched nlp_token and token_text\n",
      "unmatched nlp_token and token_text\n",
      "unmatched nlp_token and token_text\n",
      "unmatched nlp_token and token_text\n",
      "unmatched nlp_token and token_text\n",
      "unmatched nlp_token and token_text\n",
      "unmatched nlp_token and token_text\n",
      "unmatched nlp_token and token_text\n",
      "unmatched nlp_token and token_text\n",
      "unmatched nlp_token and token_text\n",
      "unmatched nlp_token and token_text\n",
      "unmatched nlp_token and token_text\n",
      "unmatched nlp_token and token_text\n",
      "unmatched nlp_token and token_text\n",
      "unmatched nlp_token and token_text\n",
      "unmatched nlp_token and token_text\n",
      "unmatched nlp_token and token_text\n",
      "<class 'AttributeError'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'dep_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_feat_4sents \u001b[38;5;241m=\u001b[39m extract_features_sent(preprocessed_train)\n\u001b[1;32m      2\u001b[0m test_feat_4sents \u001b[38;5;241m=\u001b[39m extract_features_sent(preprocessed_test)\n",
      "Cell \u001b[0;32mIn[33], line 16\u001b[0m, in \u001b[0;36mextract_features_sent\u001b[0;34m(preprocessed_list)\u001b[0m\n\u001b[1;32m     13\u001b[0m sent \u001b[38;5;241m=\u001b[39m sent_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# print(sent)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# break\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m token_deps \u001b[38;5;241m=\u001b[39m dep_path_2lemma_c (sent_dict ,nlp)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# print(token_deps)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#write the feature into token_dict\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[31], line 64\u001b[0m, in \u001b[0;36mdep_path_2lemma_c\u001b[0;34m(s, nlp)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;167;01mAttributeError\u001b[39;00m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# path from the target predict to the ancestors\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m pred_ancestors\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m,(t_pred, t_pred\u001b[38;5;241m.\u001b[39mdep_))\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Create path to the first common ancestor\u001b[39;00m\n\u001b[1;32m     67\u001b[0m common_ancs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(token_ancestors)\u001b[38;5;241m.\u001b[39mintersection(pred_ancestors)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'dep_'"
     ]
    }
   ],
   "source": [
    "train_feat_4sents = extract_features_sent(preprocessed_train)\n",
    "test_feat_4sents = extract_features_sent(preprocessed_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a14587a-3f24-4e00-bfcd-4cc65dcfdc27",
   "metadata": {},
   "source": [
    "## Extract training features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f4700c5d-01c0-4631-b050-d4bc5c31c8f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_feature_label(preprocessed_list, selected_features):\n",
    "    \"\"\"\n",
    "    Extract features and labels for training, keeping only selected features.\n",
    "    \n",
    "    Parametser:\n",
    "    preprocessed_list (list): List of lists containing token dictionaries.\n",
    "    selected_features (list): List of feature names to be kept in the extracted features.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: A tuple containing two lists - features and labels.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    gold_labels = []\n",
    "\n",
    "    # Flatten the list of lists into a single list of token dictionaries\n",
    "    token_list = [token_dict for sent_list in preprocessed_list for token_dict in sent_list]\n",
    "\n",
    "    for token_dict in token_list:\n",
    "        newdict = {feature: token_dict[feature] for feature in selected_features if feature in token_dict}\n",
    "        features.append(newdict)\n",
    "        gold_labels.append(token_dict[\"ARG\"])\n",
    "\n",
    "    return features, gold_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9690e88b-14ae-4af3-aee9-07280cd7b192",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_features = [\"upos\", \"dep_path_lemma\"]\n",
    "train_features, train_gold = extract_feature_label(preprocessed_try, selected_features)\n",
    "test_features, test_gold = extract_feature_label(preprocessed_try, selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "40ca716e-40b9-4774-8d3f-e1c9be43d576",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'upos': 'PROPN', 'dep_path_lemma': 'compound_npadvmod_kill'}, {'upos': 'PUNCT', 'dep_path_lemma': 'punct_npadvmod_kill'}] ['_', '_']\n"
     ]
    }
   ],
   "source": [
    "print(training_features[:2], gold_labels[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6100fe1d-c57b-45a1-a0b8-e18f530944a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35890733-5fd5-45e9-9097-1d61ac170de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fa840c-51ad-4088-a656-87d79ce06fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_log_model(train_features, gold_labels, max_iter):\n",
    "    logreg = LogisticRegression(max_iter=max_iter)\n",
    "    vec = DictVectorizer()\n",
    "    features_vectorized = vec.fit_transform(train_features)\n",
    "    model = logreg.fit(features_vectorized, train_targets)\n",
    "    return model, vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4f7ae6-4c08-4e80-b2fd-696d6ee0abda",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_single, vec_single = create_log_classifier(training_features, gold_labels, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c4bddb-a46e-4f4c-9d9e-dcd08ad65686",
   "metadata": {},
   "source": [
    "## model predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca838ba-1bcd-4ddd-8c79-8f8fc17070a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_data(model, vec, test_features):  \n",
    "    features = vec.transform(test_features)\n",
    "    predictions = model.predict(test_features)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1acc58-f3aa-4e3c-b40e-41ca7b585a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_pred = classify_data(model_single, vec_single, test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49051f29-9a9c-47cd-b0aa-df046ac5b8a1",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa64d165-f956-4399-999f-2c4afc5113be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ce7e99-0c04-4726-850f-df8c2201828b",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_report = classification_report(test_gold, single_pred, digits = 7, target_names = [\"True\", \"False\"])\n",
    "print(overall_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402a6b68-1518-4230-8cbd-2a1d4dea55ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44627206-a22e-4bbd-b0b5-37e9976095c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d27c90a-4e38-47a4-91f1-7428feaa67ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae160a1-f28d-48fd-83a2-4da2fceb973f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a321620e-24b7-4a5d-96ea-ed5a1a0fdfe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c689b20-9ad1-426f-9a18-9f68fdf92836",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df8788b-b20a-419e-84da-c90bb488bdff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170f6ad9-b812-4d5a-96eb-fed9c5694963",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2973b4ff-2336-4a6e-8551-73a91df8f32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_set = set(sorted(test_gold))\n",
    "label_report = classification_report(test_gold, single_pred, digits = 7, target_names = label_set)\n",
    "print(label_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f4f35024-5bea-433c-bb36-ef00b2ed7462",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'ID': '1', 'form': 'Al', 'lemma': 'Al', 'upos': 'PROPN', 'xpos': 'NNP', 'feats': 'Number=Sing', 'head': '0', 'deprel': 'root', 'deps': '0:root', 'misc': 'SpaceAfter=No', 'pred': '_', 'dup': 0, 'V': '_', 'ARG': '_', 'dep_path_lemma': 'compound_npadvmod_kill'}, {'ID': '2', 'form': '-', 'lemma': '-', 'upos': 'PUNCT', 'xpos': 'HYPH', 'feats': '_', 'head': '1', 'deprel': 'punct', 'deps': '1:punct', 'misc': 'SpaceAfter=No', 'pred': '_', 'dup': 0, 'V': '_', 'ARG': '_', 'dep_path_lemma': 'punct_npadvmod_kill'}, {'ID': '3', 'form': 'Zaman', 'lemma': 'Zaman', 'upos': 'PROPN', 'xpos': 'NNP', 'feats': 'Number=Sing', 'head': '1', 'deprel': 'flat', 'deps': '1:flat', 'misc': '_', 'pred': '_', 'dup': 0, 'V': '_', 'ARG': '_', 'dep_path_lemma': 'npadvmod_kill'}, {'ID': '4', 'form': ':', 'lemma': ':', 'upos': 'PUNCT', 'xpos': ':', 'feats': '_', 'head': '1', 'deprel': 'punct', 'deps': '1:punct', 'misc': '_', 'pred': '_', 'dup': 0, 'V': '_', 'ARG': '_', 'dep_path_lemma': 'punct_kill'}, {'ID': '5', 'form': 'American', 'lemma': 'american', 'upos': 'ADJ', 'xpos': 'JJ', 'feats': 'Degree=Pos', 'head': '6', 'deprel': 'amod', 'deps': '6:amod', 'misc': '_', 'pred': '_', 'dup': 0, 'V': '_', 'ARG': '_', 'dep_path_lemma': 'amod_nsubj_kill'}, {'ID': '6', 'form': 'forces', 'lemma': 'force', 'upos': 'NOUN', 'xpos': 'NNS', 'feats': 'Number=Plur', 'head': '7', 'deprel': 'nsubj', 'deps': '7:nsubj', 'misc': '_', 'pred': '_', 'dup': 0, 'ARG': 'ARG0', 'V': '_', 'dep_path_lemma': 'nsubj_kill'}, {'ID': '7', 'form': 'killed', 'lemma': 'kill', 'upos': 'VERB', 'xpos': 'VBD', 'feats': 'Mood=Ind|Tense=Past|VerbForm=Fin', 'head': '1', 'deprel': 'parataxis', 'deps': '1:parataxis', 'misc': '_', 'pred': 'kill.01', 'dup': 0, 'V': 'V', 'ARG': '_', 'dep_path_lemma': '_kill'}, {'ID': '8', 'form': 'Shaikh', 'lemma': 'Shaikh', 'upos': 'PROPN', 'xpos': 'NNP', 'feats': 'Number=Sing', 'head': '7', 'deprel': 'obj', 'deps': '7:obj', 'misc': '_', 'pred': '_', 'dup': 0, 'ARG': 'ARG1', 'V': '_', 'dep_path_lemma': 'compound_dobj_kill'}, {'ID': '9', 'form': 'Abdullah', 'lemma': 'Abdullah', 'upos': 'PROPN', 'xpos': 'NNP', 'feats': 'Number=Sing', 'head': '8', 'deprel': 'flat', 'deps': '8:flat', 'misc': '_', 'pred': '_', 'dup': 0, 'V': '_', 'ARG': '_', 'dep_path_lemma': 'compound_dobj_kill'}, {'ID': '10', 'form': 'al', 'lemma': 'al', 'upos': 'PROPN', 'xpos': 'NNP', 'feats': 'Number=Sing', 'head': '8', 'deprel': 'flat', 'deps': '8:flat', 'misc': 'SpaceAfter=No', 'pred': '_', 'dup': 0, 'V': '_', 'ARG': '_', 'dep_path_lemma': 'compound_dobj_kill'}, {'ID': '11', 'form': '-', 'lemma': '-', 'upos': 'PUNCT', 'xpos': 'HYPH', 'feats': '_', 'head': '8', 'deprel': 'punct', 'deps': '8:punct', 'misc': 'SpaceAfter=No', 'pred': '_', 'dup': 0, 'V': '_', 'ARG': '_', 'dep_path_lemma': 'punct_dobj_kill'}, {'ID': '12', 'form': 'Ani', 'lemma': 'Ani', 'upos': 'PROPN', 'xpos': 'NNP', 'feats': 'Number=Sing', 'head': '8', 'deprel': 'flat', 'deps': '8:flat', 'misc': 'SpaceAfter=No', 'pred': '_', 'dup': 0, 'V': '_', 'ARG': '_', 'dep_path_lemma': 'dobj_kill'}, {'ID': '13', 'form': ',', 'lemma': ',', 'upos': 'PUNCT', 'xpos': ',', 'feats': '_', 'head': '8', 'deprel': 'punct', 'deps': '8:punct', 'misc': '_', 'pred': '_', 'dup': 0, 'V': '_', 'ARG': '_', 'dep_path_lemma': 'punct_dobj_kill'}, {'ID': '14', 'form': 'the', 'lemma': 'the', 'upos': 'DET', 'xpos': 'DT', 'feats': 'Definite=Def|PronType=Art', 'head': '15', 'deprel': 'det', 'deps': '15:det', 'misc': '_', 'pred': '_', 'dup': 0, 'V': '_', 'ARG': '_', 'dep_path_lemma': 'det_appos_dobj_kill'}, {'ID': '15', 'form': 'preacher', 'lemma': 'preacher', 'upos': 'NOUN', 'xpos': 'NN', 'feats': 'Number=Sing', 'head': '8', 'deprel': 'appos', 'deps': '8:appos', 'misc': '_', 'pred': '_', 'dup': 0, 'V': '_', 'ARG': '_', 'dep_path_lemma': 'appos_dobj_kill'}, {'ID': '16', 'form': 'at', 'lemma': 'at', 'upos': 'ADP', 'xpos': 'IN', 'feats': '_', 'head': '18', 'deprel': 'case', 'deps': '18:case', 'misc': '_', 'pred': '_', 'dup': 0, 'V': '_', 'ARG': '_', 'dep_path_lemma': 'prep_appos_dobj_kill'}, {'ID': '17', 'form': 'the', 'lemma': 'the', 'upos': 'DET', 'xpos': 'DT', 'feats': 'Definite=Def|PronType=Art', 'head': '18', 'deprel': 'det', 'deps': '18:det', 'misc': '_', 'pred': '_', 'dup': 0, 'V': '_', 'ARG': '_', 'dep_path_lemma': 'det_pobj_prep_appos_dobj_kill'}, {'ID': '18', 'form': 'mosque', 'lemma': 'mosque', 'upos': 'NOUN', 'xpos': 'NN', 'feats': 'Number=Sing', 'head': '7', 'deprel': 'obl', 'deps': '7:obl:at', 'misc': '_', 'pred': '_', 'dup': 0, 'ARG': 'ARGM-LOC', 'V': '_', 'dep_path_lemma': 'pobj_prep_appos_dobj_kill'}, {'ID': '19', 'form': 'in', 'lemma': 'in', 'upos': 'ADP', 'xpos': 'IN', 'feats': '_', 'head': '21', 'deprel': 'case', 'deps': '21:case', 'misc': '_', 'pred': '_', 'dup': 0, 'V': '_', 'ARG': '_', 'dep_path_lemma': 'prep_pobj_prep_appos_dobj_kill'}, {'ID': '20', 'form': 'the', 'lemma': 'the', 'upos': 'DET', 'xpos': 'DT', 'feats': 'Definite=Def|PronType=Art', 'head': '21', 'deprel': 'det', 'deps': '21:det', 'misc': '_', 'pred': '_', 'dup': 0, 'V': '_', 'ARG': '_', 'dep_path_lemma': 'det_pobj_prep_pobj_prep_appos_dobj_kill'}, {'ID': '21', 'form': 'town', 'lemma': 'town', 'upos': 'NOUN', 'xpos': 'NN', 'feats': 'Number=Sing', 'head': '18', 'deprel': 'nmod', 'deps': '18:nmod:in', 'misc': '_', 'pred': '_', 'dup': 0, 'V': '_', 'ARG': '_', 'dep_path_lemma': 'pobj_prep_pobj_prep_appos_dobj_kill'}, {'ID': '22', 'form': 'of', 'lemma': 'of', 'upos': 'ADP', 'xpos': 'IN', 'feats': '_', 'head': '23', 'deprel': 'case', 'deps': '23:case', 'misc': '_', 'pred': '_', 'dup': 0, 'V': '_', 'ARG': '_', 'dep_path_lemma': 'prep_pobj_prep_pobj_prep_appos_dobj_kill'}, {'ID': '23', 'form': 'Qaim', 'lemma': 'Qaim', 'upos': 'PROPN', 'xpos': 'NNP', 'feats': 'Number=Sing', 'head': '21', 'deprel': 'nmod', 'deps': '21:nmod:of', 'misc': 'SpaceAfter=No', 'pred': '_', 'dup': 0, 'V': '_', 'ARG': '_', 'dep_path_lemma': 'pobj_prep_pobj_prep_pobj_prep_appos_dobj_kill'}, {'ID': '24', 'form': ',', 'lemma': ',', 'upos': 'PUNCT', 'xpos': ',', 'feats': '_', 'head': '21', 'deprel': 'punct', 'deps': '21:punct', 'misc': '_', 'pred': '_', 'dup': 0, 'V': '_', 'ARG': '_', 'dep_path_lemma': 'punct_appos_dobj_kill'}, {'ID': '25', 'form': 'near', 'lemma': 'near', 'upos': 'ADP', 'xpos': 'IN', 'feats': '_', 'head': '28', 'deprel': 'case', 'deps': '28:case', 'misc': '_', 'pred': '_', 'dup': 0, 'V': '_', 'ARG': '_', 'dep_path_lemma': 'prep_appos_dobj_kill'}, {'ID': '26', 'form': 'the', 'lemma': 'the', 'upos': 'DET', 'xpos': 'DT', 'feats': 'Definite=Def|PronType=Art', 'head': '28', 'deprel': 'det', 'deps': '28:det', 'misc': '_', 'pred': '_', 'dup': 0, 'V': '_', 'ARG': '_', 'dep_path_lemma': 'det_pobj_prep_appos_dobj_kill'}, {'ID': '27', 'form': 'Syrian', 'lemma': 'syrian', 'upos': 'ADJ', 'xpos': 'JJ', 'feats': 'Degree=Pos', 'head': '28', 'deprel': 'amod', 'deps': '28:amod', 'misc': '_', 'pred': '_', 'dup': 0, 'V': '_', 'ARG': '_', 'dep_path_lemma': 'amod_pobj_prep_appos_dobj_kill'}, {'ID': '28', 'form': 'border', 'lemma': 'border', 'upos': 'NOUN', 'xpos': 'NN', 'feats': 'Number=Sing', 'head': '21', 'deprel': 'nmod', 'deps': '21:nmod:near', 'misc': 'SpaceAfter=No', 'pred': '_', 'dup': 0, 'V': '_', 'ARG': '_', 'dep_path_lemma': 'pobj_prep_appos_dobj_kill'}, {'ID': '29', 'form': '.', 'lemma': '.', 'upos': 'PUNCT', 'xpos': '.', 'feats': '_', 'head': '1', 'deprel': 'punct', 'deps': '1:punct', 'misc': '_', 'pred': '_', 'dup': 0, 'V': '_', 'ARG': '_', 'dep_path_lemma': 'punct_kill'}]\n"
     ]
    }
   ],
   "source": [
    "for sentence in preprocessed_try:\n",
    "    sent_dict = merge_sent (sentence)\n",
    "    sent = sent_dict[\"sent\"]\n",
    "    # print(sent)\n",
    "    # break\n",
    "    token_deps = dep_path_2lemma (sent_dict ,nlp)\n",
    "    # print(token_deps)\n",
    "    \n",
    "    #write the feature into token_dict\n",
    "    try:\n",
    "        for feature, token in zip(token_deps, sentence):\n",
    "            token[\"dep_path_lemma\"] = feature\n",
    "    except IndexError: \n",
    "        # Handle the case when the zip runs out of items in one of the lists\n",
    "        for token in sentence:\n",
    "            if \"dep_path_lemma\" not in token:\n",
    "                token[\"dep_path_lemma\"] = \"_\"\n",
    "                print(f\"dep_path not extracted for {token} in {sent}\")\n",
    "\n",
    "    print(sentence)\n",
    "    \n",
    "    break\n",
    "#     new_sent_dict = []\n",
    "#     for word_dict in sentence:\n",
    "#         new_word_dict = copy.deepcopy(word_dict)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de02758-18e4-4495-9f1a-cd0cf64b29ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77881b4f-82f5-4ed1-9974-1b32cbbe8b64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sent = s[\"sent\"]\n",
    "token_text = sent.split(' ')\n",
    "doc = nlp (sent)\n",
    "t_pred = s['pred']\n",
    "print(t_pred)\n",
    "\n",
    "for t in doc:\n",
    "    if t.text==t_pred:\n",
    "        t_pred = t\n",
    "    else:\n",
    "        continue\n",
    "t_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9057eb5-8dd4-41b1-9d8b-8c7d56d587c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "killed\n"
     ]
    }
   ],
   "source": [
    "for t in doc:\n",
    "    if t.text==t_pred:\n",
    "        print(t.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c5bd266c-bbdb-4af3-8bf9-d013ec7e775e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "killed\n",
      "[['compound_npadvmod_kill', 'punct_npadvmod_kill', 'npadvmod_kill', 'punct_kill', 'amod_nsubj_kill', 'nsubj_kill', '_kill', 'compound_dobj_kill', 'compound_dobj_kill', 'compound_dobj_kill', 'punct_dobj_kill', 'dobj_kill', 'punct_dobj_kill', 'det_appos_dobj_kill', 'appos_dobj_kill', 'prep_appos_dobj_kill', 'det_pobj_prep_appos_dobj_kill', 'pobj_prep_appos_dobj_kill', 'prep_pobj_prep_appos_dobj_kill', 'det_pobj_prep_pobj_prep_appos_dobj_kill', 'pobj_prep_pobj_prep_appos_dobj_kill', 'prep_pobj_prep_pobj_prep_appos_dobj_kill', 'pobj_prep_pobj_prep_pobj_prep_appos_dobj_kill', 'punct_appos_dobj_kill', 'prep_appos_dobj_kill', 'det_pobj_prep_appos_dobj_kill', 'amod_pobj_prep_appos_dobj_kill', 'pobj_prep_appos_dobj_kill', 'punct_kill']]\n"
     ]
    }
   ],
   "source": [
    "# This function generates the dependency path from target_token to target_predicate,\n",
    "# and calculates the dependency distance of them.\n",
    "sent_deps = []\n",
    "\n",
    "for s in merged_l:\n",
    "\n",
    "    sent = s[\"sent\"]\n",
    "    token_text = sent.split(' ') #a list of tokens extracted from the original data, also the target token \n",
    "    # print(t_token)\n",
    "    doc = nlp (sent)\n",
    "    t_pred = s['pred']\n",
    "    t_lemma = s['lemma']\n",
    "    #  find the token in doc corresponding to predicate   \n",
    "    for t in doc:\n",
    "        if t.text == t_pred:\n",
    "            print(t)\n",
    "            t_pred = t  # This line stores the token in t_pred if it matches\n",
    "        else:\n",
    "            continue  \n",
    "            \n",
    "    # dep_p = t_pred.dep_\n",
    "    # print(dep_p)\n",
    "        \n",
    "            \n",
    "# find the corresponding token             \n",
    "    t_token = None\n",
    "    \n",
    "    token_deps = []\n",
    "    for i, t in enumerate(doc):\n",
    "        if t.text == token_text[i]: #check if the spacy tokenization same with the token_text in the dataset\n",
    "            t_token = t\n",
    "            # print(t_token)\n",
    "        else:\n",
    "            print(f\"different tokenization:{t} and {token_text[i]} in{sent}\")\n",
    "            continue\n",
    "            \n",
    "# #     find the dependency label of the current token\n",
    "#         dep_t = t_token.dep_\n",
    "\n",
    "# find the common ancestor\n",
    "# #find all the ancestors of each token\n",
    "        token_ancestors=[]#tuple contains the ( one ancster and the dependecy label)\n",
    "        for anc in t_token.ancestors:\n",
    "            token_ancestors.append((anc, anc.dep_))\n",
    "#             path from the target token to the ancesters\n",
    "        token_ancestors.insert(0,(t_token, t_token.dep_))\n",
    "            \n",
    "        pred_ancestors = []\n",
    "        for anc in t_pred.ancestors:\n",
    "            pred_ancestors.append((anc, anc.dep_))\n",
    "#             path from the target pred to the ancesters\n",
    "        pred_ancestors.insert(0,(t_pred, t_pred.dep_))\n",
    "    \n",
    "        common_ancs = set(token_ancestors).intersection(pred_ancestors)\n",
    "#         common_anc = common_ancs[0] #first common anc to generate shortest path\n",
    "        \n",
    "#         # Create path to the first common ancestor\n",
    "        token_path = []\n",
    "        pred_path = []\n",
    "        \n",
    "        for t in token_ancestors: #add token and dep_ until meets the common_anc\n",
    "            if t in common_ancs:\n",
    "                break\n",
    "            token_path.append(t) \n",
    "            \n",
    "        for t in pred_ancestors:\n",
    "            if t in common_ancs:\n",
    "                break\n",
    "            pred_path.append(t)\n",
    "            \n",
    "#             revers pred_path for order \"t_taget - t_pred\"\n",
    "        pred_path.reverse()\n",
    "    \n",
    "        dep_path_l= token_path + pred_path\n",
    "        if dep_path_l == []:\n",
    "            dep_path_l.append((t_token,''))\n",
    "        \n",
    "        dep_path = ''\n",
    "        for t,p in dep_path_l:\n",
    "            dep_path += p + '_' \n",
    "        \n",
    "#         add the predicate lemma\n",
    "        dep_path += t_lemma\n",
    "        # print(dep_path)\n",
    "        # print(type(dep_path))\n",
    "        token_deps.append(dep_path)\n",
    "        \n",
    "    sent_deps.append(token_deps)\n",
    "    print(sent_deps)\n",
    "    break\n",
    "    \n",
    "    \n",
    "# # check the number of features and tokens\n",
    "#     if len(token_deps) == len(token_text):\n",
    "#         for t, f in zip(token_text, token_deps):\n",
    "#             print(t, f)\n",
    "#     else:\n",
    "#         print(\"feature amount and token anount not match correctly\" )\n",
    "        \n",
    "            \n",
    "\n",
    "        \n",
    "#     break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ce87a804-3548-4e7c-8b81-122d2fc1a023",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Al compound Zaman PROPN [] [Zaman, killed]\n",
      "- punct Zaman PROPN [] [Zaman, killed]\n",
      "Zaman npadvmod killed VERB [Al, -] [killed]\n",
      ": punct killed VERB [] [killed]\n",
      "American amod forces NOUN [] [forces, killed]\n",
      "forces nsubj killed VERB [American] [killed]\n",
      "killed ROOT killed VERB [Zaman, :, forces, Ani, .] []\n",
      "Shaikh compound Ani PROPN [] [Ani, killed]\n",
      "Abdullah compound Ani PROPN [] [Ani, killed]\n",
      "al compound Ani PROPN [] [Ani, killed]\n",
      "- punct Ani PROPN [] [Ani, killed]\n",
      "Ani dobj killed VERB [Shaikh, Abdullah, al, -, ,, preacher] [killed]\n",
      ", punct Ani PROPN [] [Ani, killed]\n",
      "the det preacher ADJ [] [preacher, Ani, killed]\n",
      "preacher appos Ani PROPN [the, at, ,, near] [Ani, killed]\n",
      "at prep preacher ADJ [mosque] [preacher, Ani, killed]\n",
      "the det mosque NOUN [] [mosque, at, preacher, Ani, killed]\n",
      "mosque pobj at ADP [the, in] [at, preacher, Ani, killed]\n",
      "in prep mosque NOUN [town] [mosque, at, preacher, Ani, killed]\n",
      "the det town NOUN [] [town, in, mosque, at, preacher, Ani, killed]\n",
      "town pobj in ADP [the, of] [in, mosque, at, preacher, Ani, killed]\n",
      "of prep town NOUN [Qaim] [town, in, mosque, at, preacher, Ani, killed]\n",
      "Qaim pobj of ADP [] [of, town, in, mosque, at, preacher, Ani, killed]\n",
      ", punct preacher ADJ [] [preacher, Ani, killed]\n",
      "near prep preacher ADJ [border] [preacher, Ani, killed]\n",
      "the det border NOUN [] [border, near, preacher, Ani, killed]\n",
      "Syrian amod border NOUN [] [border, near, preacher, Ani, killed]\n",
      "border pobj near ADP [the, Syrian] [near, preacher, Ani, killed]\n",
      ". punct killed VERB [] [killed]\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "            [child for child in token.children], [ans for ans in token.ancestors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5859a26-6cf4-4785-bd5d-eb8122224d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence, negation cue, token\n",
    "sentence, predicate, current_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee02227-c2c5-4e26-a8bf-5304cc3bddab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fed214-8c3e-42de-8c46-243c7c6a6e53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "64896f5c-ade0-40c1-bc6d-1b9f25982504",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', 'Al', 'Al', 'PROPN', 'NNP', 'Number=Sing', '0', 'root', '0:root', 'SpaceAfter=No', '_', '_']\n"
     ]
    }
   ],
   "source": [
    "# for i in range(len(try_l[0])):#try_l[0] is a sentence\n",
    "#     token_l_list = [] #contain labels of one token\n",
    "#     for t_l in try_l[0][i]:#token list with the labels, t_l: label\n",
    "#         token_l_list.append(str(t_l))\n",
    "    \n",
    "#     #extract first 11 columns of shared information from dataset\n",
    "#     try:\n",
    "#         tokendict = {\"ID\":token_l_list[0], \"form\":token_l_list[1], \"lemma\":token_l_list[2], \"upos\":token_l_list[3], \"xpos\":token_l_list[4], \"feats\":token_l_list[5], \"head\":token_l_list[6], \n",
    "#                              \"deprel\":token_l_list[7], \"deps\":token_l_list[8], \"misc\":token_l_list[9], \"pred\":token_l_list[10]}\n",
    "#     except IndexError:\n",
    "#         tokendict['pred'] = '_'\n",
    "        \n",
    "#     #add information depending on the predicate\n",
    "#     #sents with no predicate \n",
    "#     if len(token_l_list) <= 11: \n",
    "#         tokendict['V'], tokendict['ARG'] ,tokendict['dup'] = '_','_','_'\n",
    "#         sents[0].append(tokendict)\n",
    "        \n",
    "#     #sents with one or more than one predicate \n",
    "#     if len(token_l_list) > 11: \n",
    "#         dup = len(token_l_list)-11 # Times for dpulication\n",
    "#         for k in range(0,dup):\n",
    "#             tokendictk = copy.deepcopy(tokendict)\n",
    "#             tokendictk['dup'] = k\n",
    "#             ARGV = components[k+11]\n",
    "#             #add the information based on the position of the time of duplication\n",
    "#             if ARGV == \"V\":\n",
    "#                 tokendictk['V'],tokendictk['ARG'] = 'V','_'\n",
    "#             if (ARGV != 'V') & (ARGV != '_'):\n",
    "#                 tokendictk['ARG'],tokendictk['V'],tokendictk['pred'] = ARGV,'_','_'\n",
    "#             if ARGV == '_':\n",
    "#                 tokendictk['V'],tokendictk['ARG'],tokendictk['pred'] = '_','_','_'\n",
    "#                 sents[k].append(tokendictk)\n",
    "                \n",
    "            \n",
    "#         res = [ele for ele in sents if ele != []] # remove empty list\n",
    "#         sentlist += res\n",
    "        \n",
    "#         tokendict['V'], tokendict['ARG'] ,tokendict['dup'] = '_','_','_'\n",
    "#         sents[0].append(tokendict)\n",
    "#     print(token_l_list)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cf8c24ab-aa1a-435b-883d-861f80e97c0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', 'Al', 'Al', 'PROPN', 'NNP', 'Number=Sing', '0', 'root', '0:root', 'SpaceAfter=No', '_', '_']\n",
      "['2', '-', '-', 'PUNCT', 'HYPH', '_', '1', 'punct', '1:punct', 'SpaceAfter=No', '_', '_']\n",
      "['3', 'Zaman', 'Zaman', 'PROPN', 'NNP', 'Number=Sing', '1', 'flat', '1:flat', '_', '_', '_']\n",
      "['4', ':', ':', 'PUNCT', ':', '_', '1', 'punct', '1:punct', '_', '_', '_']\n",
      "['5', 'American', 'american', 'ADJ', 'JJ', 'Degree=Pos', '6', 'amod', '6:amod', '_', '_', '_']\n",
      "['6', 'forces', 'force', 'NOUN', 'NNS', 'Number=Plur', '7', 'nsubj', '7:nsubj', '_', '_', 'ARG0']\n",
      "['7', 'killed', 'kill', 'VERB', 'VBD', 'Mood=Ind|Tense=Past|VerbForm=Fin', '1', 'parataxis', '1:parataxis', '_', 'kill.01', 'V']\n",
      "['8', 'Shaikh', 'Shaikh', 'PROPN', 'NNP', 'Number=Sing', '7', 'obj', '7:obj', '_', '_', 'ARG1']\n",
      "['9', 'Abdullah', 'Abdullah', 'PROPN', 'NNP', 'Number=Sing', '8', 'flat', '8:flat', '_', '_', '_']\n",
      "['10', 'al', 'al', 'PROPN', 'NNP', 'Number=Sing', '8', 'flat', '8:flat', 'SpaceAfter=No', '_', '_']\n",
      "['11', '-', '-', 'PUNCT', 'HYPH', '_', '8', 'punct', '8:punct', 'SpaceAfter=No', '_', '_']\n",
      "['12', 'Ani', 'Ani', 'PROPN', 'NNP', 'Number=Sing', '8', 'flat', '8:flat', 'SpaceAfter=No', '_', '_']\n",
      "['13', ',', ',', 'PUNCT', ',', '_', '8', 'punct', '8:punct', '_', '_', '_']\n",
      "['14', 'the', 'the', 'DET', 'DT', 'Definite=Def|PronType=Art', '15', 'det', '15:det', '_', '_', '_']\n",
      "['15', 'preacher', 'preacher', 'NOUN', 'NN', 'Number=Sing', '8', 'appos', '8:appos', '_', '_', '_']\n",
      "['16', 'at', 'at', 'ADP', 'IN', '_', '18', 'case', '18:case', '_', '_', '_']\n",
      "['17', 'the', 'the', 'DET', 'DT', 'Definite=Def|PronType=Art', '18', 'det', '18:det', '_', '_', '_']\n",
      "['18', 'mosque', 'mosque', 'NOUN', 'NN', 'Number=Sing', '7', 'obl', '7:obl:at', '_', '_', 'ARGM-LOC']\n",
      "['19', 'in', 'in', 'ADP', 'IN', '_', '21', 'case', '21:case', '_', '_', '_']\n",
      "['20', 'the', 'the', 'DET', 'DT', 'Definite=Def|PronType=Art', '21', 'det', '21:det', '_', '_', '_']\n",
      "['21', 'town', 'town', 'NOUN', 'NN', 'Number=Sing', '18', 'nmod', '18:nmod:in', '_', '_', '_']\n",
      "['22', 'of', 'of', 'ADP', 'IN', '_', '23', 'case', '23:case', '_', '_', '_']\n",
      "['23', 'Qaim', 'Qaim', 'PROPN', 'NNP', 'Number=Sing', '21', 'nmod', '21:nmod:of', 'SpaceAfter=No', '_', '_']\n",
      "['24', ',', ',', 'PUNCT', ',', '_', '21', 'punct', '21:punct', '_', '_', '_']\n",
      "['25', 'near', 'near', 'ADP', 'IN', '_', '28', 'case', '28:case', '_', '_', '_']\n",
      "['26', 'the', 'the', 'DET', 'DT', 'Definite=Def|PronType=Art', '28', 'det', '28:det', '_', '_', '_']\n",
      "['27', 'Syrian', 'syrian', 'ADJ', 'JJ', 'Degree=Pos', '28', 'amod', '28:amod', '_', '_', '_']\n",
      "['28', 'border', 'border', 'NOUN', 'NN', 'Number=Sing', '21', 'nmod', '21:nmod:near', 'SpaceAfter=No', '_', '_']\n",
      "['29', '.', '.', 'PUNCT', '.', '_', '1', 'punct', '1:punct', '_', '_', '_']\n",
      "['1', '[', '[', 'PUNCT', '-LRB-', '_', '10', 'punct', '10:punct', 'SpaceAfter=No', '_', '_', '_', '_', '_']\n",
      "['2', 'This', 'this', 'DET', 'DT', 'Number=Sing|PronType=Dem', '3', 'det', '3:det', '_', '_', '_', '_', '_', '_']\n",
      "['3', 'killing', 'killing', 'NOUN', 'NN', 'Number=Sing', '10', 'nsubj', '10:nsubj', '_', 'kill.01', 'V', '_', 'ARG0', '_']\n",
      "['4', 'of', 'of', 'ADP', 'IN', '_', '7', 'case', '7:case', '_', '_', '_', '_', '_', '_']\n",
      "['5', 'a', 'a', 'DET', 'DT', 'Definite=Ind|PronType=Art', '7', 'det', '7:det', '_', '_', '_', '_', '_', '_']\n",
      "['6', 'respected', 'respected', 'ADJ', 'JJ', 'Degree=Pos', '7', 'amod', '7:amod', '_', '_', '_', '_', '_', '_']\n",
      "['7', 'cleric', 'cleric', 'NOUN', 'NN', 'Number=Sing', '3', 'nmod', '3:nmod:of', '_', '_', 'ARG1', '_', '_', '_']\n",
      "['8', 'will', 'will', 'AUX', 'MD', 'VerbForm=Fin', '10', 'aux', '10:aux', '_', '_', '_', '_', 'ARGM-MOD', '_']\n",
      "['9', 'be', 'be', 'AUX', 'VB', 'VerbForm=Inf', '10', 'aux', '10:aux', '_', 'be.03', '_', 'V', '_', '_']\n",
      "['10', 'causing', 'cause', 'VERB', 'VBG', 'VerbForm=Ger', '0', 'root', '0:root', '_', 'cause.01', '_', '_', 'V', '_']\n",
      "['11', 'us', 'we', 'PRON', 'PRP', 'Case=Acc|Number=Plur|Person=1|PronType=Prs', '10', 'iobj', '10:iobj', '_', '_', '_', '_', 'ARGM-GOL', '_']\n",
      "['12', 'trouble', 'trouble', 'NOUN', 'NN', 'Number=Sing', '10', 'obj', '10:obj', '_', '_', '_', '_', 'ARG1', '_']\n",
      "['13', 'for', 'for', 'ADP', 'IN', '_', '14', 'case', '14:case', '_', '_', '_', '_', '_', '_']\n",
      "['14', 'years', 'year', 'NOUN', 'NNS', 'Number=Plur', '10', 'obl', '10:obl:for', '_', '_', '_', '_', 'ARGM-TMP', 'ARG1']\n",
      "['15', 'to', 'to', 'PART', 'TO', '_', '16', 'mark', '16:mark', '_', '_', '_', '_', '_', '_']\n",
      "['16', 'come', 'come', 'VERB', 'VB', 'VerbForm=Inf', '14', 'acl', '14:acl:to', 'SpaceAfter=No', 'come.01', '_', '_', '_', 'V']\n",
      "['17', '.', '.', 'PUNCT', '.', '_', '10', 'punct', '10:punct', 'SpaceAfter=No', '_', '_', '_', '_', '_']\n",
      "['18', ']', ']', 'PUNCT', '-RRB-', '_', '10', 'punct', '10:punct', '_', '_', '_', '_', '_', '_']\n",
      "['1', 'Two', 'two', 'NUM', 'CD', 'NumType=Card', '6', 'nsubj:pass', '6:nsubj:pass', '_', '_', '_', '_', 'ARG1']\n",
      "['2', 'of', 'of', 'ADP', 'IN', '_', '3', 'case', '3:case', '_', '_', '_', '_', '_']\n",
      "['3', 'them', 'they', 'PRON', 'PRP', 'Case=Acc|Number=Plur|Person=3|PronType=Prs', '1', 'nmod', '1:nmod:of', '_', '_', '_', '_', '_']\n",
      "['4', 'were', 'be', 'AUX', 'VBD', 'Mood=Ind|Tense=Past|VerbForm=Fin', '6', 'aux', '6:aux', '_', 'be.03', 'V', '_', '_']\n",
      "['5', 'being', 'be', 'AUX', 'VBG', 'VerbForm=Ger', '6', 'aux:pass', '6:aux:pass', '_', 'be.03', '_', 'V', '_']\n",
      "['6', 'run', 'run', 'VERB', 'VBN', 'Tense=Past|VerbForm=Part|Voice=Pass', '0', 'root', '0:root', '_', 'run.01', '_', '_', 'V']\n",
      "['7', 'by', 'by', 'ADP', 'IN', '_', '9', 'case', '9:case', '_', '_', '_', '_', '_']\n",
      "['8', '2', '2', 'NUM', 'CD', 'NumType=Card', '9', 'nummod', '9:nummod', '_', '_', '_', '_', '_']\n",
      "['9', 'officials', 'official', 'NOUN', 'NNS', 'Number=Plur', '6', 'obl', '6:obl:by', '_', '_', '_', '_', 'ARG0']\n",
      "['10', 'of', 'of', 'ADP', 'IN', '_', '12', 'case', '12:case', '_', '_', '_', '_', '_']\n",
      "['11', 'the', 'the', 'DET', 'DT', 'Definite=Def|PronType=Art', '12', 'det', '12:det', '_', '_', '_', '_', '_']\n",
      "['12', 'Ministry', 'Ministry', 'PROPN', 'NNP', 'Number=Sing', '9', 'nmod', '9:nmod:of', '_', '_', '_', '_', '_']\n",
      "['13', 'of', 'of', 'ADP', 'IN', '_', '15', 'case', '15:case', '_', '_', '_', '_', '_']\n",
      "['14', 'the', 'the', 'DET', 'DT', 'Definite=Def|PronType=Art', '15', 'det', '15:det', '_', '_', '_', '_', '_']\n",
      "['15', 'Interior', 'Interior', 'PROPN', 'NNP', 'Number=Sing', '12', 'nmod', '12:nmod:of', 'SpaceAfter=No', '_', '_', '_', '_']\n",
      "['16', '!', '!', 'PUNCT', '.', '_', '6', 'punct', '6:punct', '_', '_', '_', '_', '_']\n"
     ]
    }
   ],
   "source": [
    "# duplicate the sentences with multiple predicates\n",
    "\n",
    "sentlist = []\n",
    "\n",
    "for s in try_list:# s is a sentence list of token lists\n",
    "    sents = [[] for _ in range(50)]\n",
    "\n",
    "    for x in range(len(s)):#x is the token list\n",
    "        components = []\n",
    "        for f in range(len(s[x])):# f is the feature label of the token \n",
    "            components.append(str(s[x][f]))\n",
    "\n",
    "            # #add the first predicate with 11 label\n",
    "        print(components)\n",
    "        \n",
    "        for i in range(0, 10):\n",
    "            try:\n",
    "                tokendict = {\"ID\":components[0], \"form\":components[1], \"lemma\":components[2], \"upos\":components[3], \"xpos\":components[4], \"feats\":components[5], \"head\":components[6], \n",
    "                             \"deprel\":components[7], \"deps\":components[8], \"misc\":components[9], \"pred\":components[10]}\n",
    "            except IndexError: # Wrong sentence in the dataset that have no column 11\n",
    "                tokendict['pred'] = '_'\n",
    "                \n",
    "            # If sentence have no predicate: assign the values '_'\n",
    "        if len(components) <= 11: \n",
    "            tokendict['V'], tokendict['ARG'] ,tokendict['dup'] = '_','_','_'\n",
    "            sents[0].append(tokendict)\n",
    "            \n",
    "        # Sentence have one or more predicate\n",
    "        if len(components) > 11: \n",
    "            dup = len(components)-11 # Times for dpulication\n",
    "            for k in range (0, dup):\n",
    "                    tokendictk = copy.deepcopy(tokendict)\n",
    "                    tokendictk['dup'] = k\n",
    "                    ARGV = components[k+11]\n",
    "                    if ARGV == \"V\":\n",
    "                        tokendictk['V'],tokendictk['ARG'] = 'V','_'\n",
    "                        try:\n",
    "                            tokendictk['pred'] = s[int(tokendictk['ID'])-1][10]\n",
    "                        except IndexError:\n",
    "                            print('The following sentence contains error:',sentence)\n",
    "                            continue\n",
    "                    if (ARGV != 'V') & (ARGV != '_'):\n",
    "                        tokendictk['ARG'],tokendictk['V'],tokendictk['pred'] = ARGV,'_','_'\n",
    "                    if ARGV == '_':\n",
    "                        tokendictk['V'],tokendictk['ARG'],tokendictk['pred'] = '_','_','_'\n",
    "                    sents[k].append(tokendictk)\n",
    "                    \n",
    "            \n",
    "        res = [ele for ele in sents if ele != []] # remove empty list\n",
    "        sentlist += res                                                                                                                                                                                                                                                                                      \n",
    "\n",
    "\n",
    "        \n",
    "       \n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28d523f-613c-4124-9d82-536a83ba3a1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
